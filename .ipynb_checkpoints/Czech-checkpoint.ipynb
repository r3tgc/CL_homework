{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм Витерби для чешского корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки, используем третий питон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # парсилка корпуса\n",
    "from io import open\n",
    "import pickle # для сохранения состояния обучения\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем тренировочный и тестовый корпус из ссылок https://docs.google.com/spreadsheets/d/1fTPyKT3-lVa1rL69jMz2wdWMLWoAEixDxlEUfiorA2k/edit?usp=drive_web&ouid=110689254510656065381\n",
    "Корпус в формате conllu - текстовый файл с текстом на нужном языке- список предложений ,где у каждого слова указана его простая форма(лемма) и часть речи (глагол,прилагательное,знак препинания и тд) -тэг, и еще много всякой инфы,которая нам не нужна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_in = \"./UD_Czech-PDT/\" # директория с корпусами \n",
    "train_file = \"cs_pdt-ud-train-l.conllu\" # тренировочный корпус\n",
    "test_file = \"cs_pdt-ud-test.conllu\" # тестовый корпус"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открываем файл с тренировочным корпусом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = conllu.parse_incr(open(dir_in+train_file,'r',encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитываем количество триграммов,биграммов и монограммов в корпусе,используя словари. Триграмм - три слова подряд,биграмм - два, монограмм - одно. Т.е. количество всевозможных сочетаний из всего корпуса по три,два и одному слову.Если брать все различные слова и найти для них всевозможные сочетания,получится дофига,поэтому подсчитываем только те что есть в нашем тексте "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# т.е. набираем статистику по тексту,по сути это и есть обучение..\n",
    "c3 = {} # количество различных триграммов ,которые встречаются в нашем тексте\n",
    "c2 = {} # //---//---// биграммов\n",
    "c1 = {} # //---//---// монограммов\n",
    "cT = {} # //---//---// пар слово-тег(количество различных слов со своими тегами,т.е. собака-существительное) \n",
    "for tokenlist in data_train: # идем по всем предложениям в тексте(каждое предложение - список слов,\n",
    "    # с указанными леммами(lemma) и частями речи-тегами(upostag)\n",
    "    sentence = [[\"*\",\"*\"]] + [[\"*\",\"*\"]] + [[tokenlist[i][\"lemma\"], tokenlist[i][\"upostag\"]]# список списков\n",
    "        for i in range(len(tokenlist)) ] + [[\"STOP\",\"STOP\"]] # склеиваем предложение  как список пар [лемма,тег]\n",
    "# добавляя в начало индикатор начала [\"*\",\"*\"] и конца [\"STOP\",\"STOP\"]\n",
    "#теперь идем по этому предложению\n",
    "    for i in range(0, len(sentence)-2):\n",
    "        # считаем количество уникальных триграммов  \n",
    "        c3old = c3.get( (sentence[i][1], sentence[i+1][1], sentence[i+2][1]), 0)# если такого триграмма еще нет в словаре,вернуть 0\n",
    "        c3.update( { (sentence[i][1], sentence[i+1][1], sentence[i+2][1]): c3old+1 } ) #если есть,увеличиваем количество на 1\n",
    "    # аналогично для биграммов\n",
    "    for i in range(0, len(sentence)-1):\n",
    "        c2old = c2.get( (sentence[i][1], sentence[i+1][1]), 0)\n",
    "        c2.update( { (sentence[i][1], sentence[i+1][1]): c2old+1 } )\n",
    "    # просто количество различных слов\n",
    "    for i in range(0, len(sentence)):\n",
    "        c1old = c1.get( (sentence[i][1]), 0)\n",
    "        c1.update( { (sentence[i][1]): c1old+1 } )\n",
    "    # количество различных слов(sentence[i][1]) привязанных к определенному тегу (sentence[i][0]),\n",
    "     # одно и то же слово по идее может встретиться с разными тегами,и это будут разные пары\n",
    "    for i in range(0, len(sentence)):\n",
    "        cTold = cT.get( (sentence[i][1], sentence[i][0]), 0)\n",
    "        cT.update( { (sentence[i][1], sentence[i][0]): cTold+1 } )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем данные в пикл, чтобы потом использовать, не обучая заново"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3050\n",
      "275\n",
      "20\n",
      "40242\n"
     ]
    }
   ],
   "source": [
    "print (len(c3))\n",
    "print (len(c2))\n",
    "print (len(c1))\n",
    "print (len(cT))\n",
    "\n",
    "# Saving:\n",
    "with open('cs_czech.pkl', 'wb') as f:\n",
    "    pickle.dump([cT, c1, c2, c3], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные из файла обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting back the cs:\n",
    "with open('cs_czech.pkl', 'rb') as f:\n",
    "    cT, c1, c2, c3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(c1.keys())# список уникальных тегов(все теги которые попались в тексте)\n",
    " #  убираем теги которые обозначают начало и конец из этого списка,для анализа оно нам не надо\n",
    "tags.remove('*') \n",
    "tags.remove('STOP')\n",
    "k = len(tags) # количество уникальных тегов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательные функции из алгоритма Витерби"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AUX', 'SCONJ', 'PUNCT', 'NUM', 'PART', 'DET', 'NOUN', 'ADJ', 'VERB', 'SYM', '_', 'X', 'ADV', 'PRON', 'PROPN', 'INTJ', 'ADP', 'CCONJ']\n"
     ]
    }
   ],
   "source": [
    "def q(s, u, v):  # некая функция описанная в книге ,для каждой тройки слов (s,u,v) она равна с3/с2\n",
    "     # где с3(u, v, s) - сколько раз такая тройка встретилась в тексте, с2(u, v) - сколько раз такая двойка встретилась\n",
    "    # функция гет у словаря вернет eps, если не найдет такие тройки и двойки в нашей статистике\n",
    "    # (в оригинальном алгоритме будет 0),но нам не надо делить ноль но ноль\n",
    "    eps = 1e-7\n",
    "    return c3.get((u, v, s), eps) / c2.get((u, v), eps)\n",
    "\n",
    "\n",
    "def e(x, s): #  тоже некая функция из книги которую дал дурандин,только тут количество  сколько раз конкретная пара слово-тег\n",
    "    #  встретилась в тексте деленное на сколько раз встретилось слово s и с другими тегами тоже\n",
    "    eps = 1e-7\n",
    "    return cT.get((s, x), eps) / c1.get((s), eps)\n",
    "\n",
    " # для каждого слова нам надо предсказать тег в зависимости от предыдущих тегов,\n",
    "     # поэтому набор возможных тегов для каждого слова будем искать так\n",
    "def K(n): # выдает возможные тэги в зависимости от положения слова в предложении (в начале предложения - *) \n",
    "    if n == -1 or n == 0: # поскольку алгоритм начинает расчеты с первого слова и смотрит теги на два шага назад,\n",
    "         # то для первого слова будем возвращать звездочки\n",
    "        return ['*']\n",
    "    else:\n",
    "         # для всех слов кроме первого теги могут быть любыми из списка всех тегов встретившихся в корпусе\n",
    "        return tags \n",
    "\n",
    "print(K(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сам алгоритм Витерби в виде функции от токенизированного предложения ,возвращающей список тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent):# принимает предложение и находит для каждого слова наиболее вероятный тег в зависимости от двух предыдущих тегов\n",
    "    # инициализируем \n",
    "    Pi = {(0, '*', '*'): 1} # старт - для первого слова вероятность что два предыдущих это стартовый символ *\n",
    "    # ,это сто пудов,=1)\n",
    "    bp = {}\n",
    "    n = len(sent) - 2 #  для последнего слова посчитаем отдельно\n",
    "    y = [\"\"] * (n + 1)\n",
    "    # идем по словам в предложении\n",
    "    for k in range(1, n + 1):\n",
    "        xk = sent[k]\n",
    "        \n",
    "        for u in K(k-1):# смотрим по всем возможным тегам для предыдущего слова\n",
    "            for v in K(k): # по всем возможным тегам для текущего слова\n",
    "                w = K(k-2) # список возможных тегов для предпредыдущего слова,\n",
    "                v1 = map(lambda wi:\n",
    "                            Pi.get((k-1, wi, u)) *\n",
    "                            q(v, wi, u) *\n",
    "                            e(xk, v), w)\n",
    "                v1 = list(v1) #выше посчитали типа вероятности для каждого тега\n",
    "                PiNew = max(v1) # выбираем тег с наибольшей вероятностью - вот и все предсказание\n",
    "                bpNew = w[v1.index(PiNew)] #тег тоже запомним\n",
    "                Pi.update({(k, u, v): PiNew}) # обновляем вероятность максимальным значением\n",
    "                bp.update({(k, u, v): bpNew}) # и тег,который ей соответствует\n",
    "\n",
    "    v2 = {}\n",
    "    for u in K(n-1): #  отдельно считаем крайний случай, для последнего слова\n",
    "        for v in K(n):\n",
    "            v2.update({(,n u, v): Pi.get((n, u, v)) * q(\"STOP\", u, v)})\n",
    "\n",
    "    v2max = max(list(v2.values())) #  берем максимальную вероятность\n",
    "    for (n, u, v), val in v2.items(): #  ищем тройку тегов которя ей соответствует\n",
    "        if val == v2max:\n",
    "            (y[n-1], y[n]) = (u,v) #  добавляем в ответ\n",
    "\n",
    "    for k in range(n-2, 0, -1): #   добавляем в ответ остальные теги\n",
    "        y[k] = bp.get((k + 2, y[k+1], y[k+2]))\n",
    "\n",
    "    #print(y[1:])\n",
    "    return y[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открываем тестовый корпус и прогоняем по нему алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_test = conllu.parse_incr(open(dir_in+test_file,'r',encoding = \"utf-8\"))\n",
    "test_tags = [] # оригинальные теги корпуса\n",
    "predict_tags = [] # предсказанные алгоритмом\n",
    "\n",
    "for tokenlist in data_test:#  идем по тестовому корпусу\n",
    "    #  заполняем каждое предложение как нам надо\n",
    "    sentence=[\"*\"] + [tokenlist[i][\"lemma\"] for i in range(len(tokenlist)) ] + [\"STOP\"]\n",
    "    #  отдельно правильные теги\n",
    "    test_tags.append([tokenlist[i][\"upostag\"] for i in range(len(tokenlist)) ])\n",
    "    #  предсказываем теги для предложения\n",
    "    predict_tags.append(viterbi(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем ошибку как отношение количества неправильно предсказанных тегов к общему числу тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.05380163674132475\n"
     ]
    }
   ],
   "source": [
    "err = 0.\n",
    "for sent in range(len(test_tags)):\n",
    "    err += np.mean(np.array(test_tags[sent]) != np.array(predict_tags[sent]))\n",
    "print('Error = ', err/len(test_tags))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
